---
title: 'AN(C)OVA in R'
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    df_print: paged
    theme: default
bibliography: /Users/henrikeckermann/Documents/workspace/bibTex/packages.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r echo=FALSE}
library(papaja)
```


## Introduction
In ANOVA, the goal is to compare means of an outcome variable between groups when we have more than 2 groups. We do not test between which groups the differences are but rather if there is a difference at all. We could test which group means differ by using posthoc tests when using the frequentist approach (more on that later). This article assumes that you have background knowledge in linear models, especially fitting linear regression models in R. ANOVA and also ANCOVA can be described as special cases of a linear regression model, where e.g. a one way ANOVA is simply a linear regression with a categorical predictor. I want to acknowledge the authors of @Korner-Nievergelt2015 because for this article I use in part their code examples, especially when it comes to the simulation of data and the intro to the Bayesian framework when it comes to ANOVA. I will also rely on principles that have been formulated by @Gelman2007.

## Behind the scenes
To explain the procedure, I will compare it with what we are used to when fitting a linear regression model: In linear regression, we compare whether the linear model that we use to create the regression line, fits the data better than the baseline model (mean). The fit is reflected by the variance around the mean and the regression line, respectively. 

#### SST, SSW and SSB
The variance of the data around our basic model (mean) is called the _total sum of squares_ and is defined as:  

$$SST = \sum_{i=1}^n (y_i-\bar{y})^2$$

The variance of the data around the 'better' model or stated differently: The variance that the better model still cannot explain, is the _residual sum of squares_ and is defined as:

$$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$ 

In the context of ANOVA, this is often referred to as _sum of squares within_ (SSW). As in ANOVA the model is the corresponding group mean, we can write more specifically:  

$$SSW=\sum_{i=1}^n(y_i-\bar{y}_{gi})^2$$

If we subtract RSS from SST, we end up with the _model sum of squares_. In the context of ANOVA, the term is called _sum of squares between_ (SSB):

$$SSB=\sum_{i=1}^n(\bar{y_{gi}}-\bar{y})^2$$

SSM/SSB reflect the part of the SST that the 'better' model can explain, whereas RSS/SSW reflect the part of the SST that the 'better' model still cannot explain. In summary: SSB+SSW=SST.


#### Frequentist approach
Our null hypothesis is that $\theta_1 = \theta_2 = \theta_3$. If this hypothesis is true, we expect that the variance between the groups equals the variance within the groups. We calculate the mean sum of squares (MSB and MSW) by dividing SSB and SSW by their corresponding degrees of freedom, which are $df_{SSB}=k-1$ and $df_{SSW} = N-k$. Then we can calculate the F-statistic:  

$$F=\frac{MSB}{MSW}$$

Depending of the degrees of freedom ($df_{SSB}$ and $df_{SSW}$), we can check the distribution of the F-value under $H_0$ to check for the probability that we get the F-value we have or a more extreme one. At the cutoff of 0.05, we would decide that there is a least one mean that is different from the other means.


## One-Way ANOVA in R
If we fit a linear model, R automatically picks $\beta_0$ as the reference group. Thus, the intercept reflects the group mean of the first group. If we have three groups, then $\beta_1$ reflects the difference in means between the first and the second group and $\beta_2$ relfects the difference in means between the first and the third group. I will simulate data (3 groups with different means) and then illustrate this using lm in R:  

```{r}
#import libraries needed
library(arm)
library(ggplot2)
library(reshape2)
library(plyr)
#simulate data 
b0 <- 12 #true mean group 1
b1 <- 0.75 #difference in true means between group 2 to group 1
b2 <- -0.75 #difference in true means between group 1 and group 3
sigma <- 2 #residual SD (=within group SD)
n <- 90
group <- factor(rep(c(1, 2, 3), each=30))
#simulate the y variable
simresid <- rnorm(n, mean=0, sd=sigma)
y <- b0 + as.numeric(group=='2') *b1 + as.numeric(group=='3')*b2 + simresid
df <- data.frame(group, y)
#The means of the fake data
ddply(df, .(group), summarize, mean = mean(y))
#fit linear model
mod <- lm('y~group', df)
coefs <- coef(mod)
summary(mod)
#see how aov yields the same F-test for one-way A
aovmod <- aov(y~group, df)
summary(aovmod)
```

```{r}
library(psych)
describe(df)
str(df)
```

As we can see, the coefficients of our model reflect the group means as described above: $\hat{y} =$ `r coefs[1]` + `r coefs[2]`$group_2$ - `r coefs[2]`$group_3$. In the summary of our model we also find the significant F-statistic. Now we could do post hoc tests to check which groups differ (see e.g. Bonferroni or Tukey). These tests are used because if one conducts e.g. multiple t-tests, there is a 5% chance to falsely get a significant result for each test just by chance. I will not cover post hoc test at this point but will mention an advice about it that was cited in @Korner-Nievergelt2015 later.


#### A Bayesian way
Using the Bayesian framework we could ask in how far the data support the hypothesis that the mean of group 2 or group 3 is greater than mean of group 1. We could simulate the posterior distribution of the parameters and their differences. Then we could calculate the 95% credible intervals (CrI) to give an estimate. Here is how such a simulation could look like using the _arm_ package. I used different parameters for the fake data simulation to get a feeling for these tools:



I advice you to also play with the means in the data simulation a little. In the current setting, g2 is larger than g1 in `r mean(m.g2>m.g1)*100`% of all simulated cases whereas g3 is larger than g1 in `r mean(m.g3>m.g1)*100`% of the cases.  


## Two-Way ANOVA
For this part I use the dataset out of the book [@Korner-Nievergelt2015]. Let us have a look at the columns. It is self-explanatory:

```{r}
library(blmeco)
data(periparusater)
df <- periparusater
summary(df)
head(df)
```

We will compare the wing length of _Periparus ater_ between the two age groups (juvenile vs adult) and the two sexes:  

```{r}
#change names for convenience
levels(df$age) = c('juv', 'adu')
levels(df$sex) = c('male', 'female')
#fit model
pmod <- lm('wing ~ sex + age', df)
#plot means and individuals data points
ggplot(df, aes(x=sex,y=wing)) +
  geom_jitter(width=0.05) +
  stat_summary(fun.y = mean, geom = 'point', color = 'red') +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.2, col='red') +
  facet_grid(.~age)
summary(pmod)
```

We have the following model:  

$$wing = 63.84 - 3.34female + 0.88adu$$

This model would translate to: The mean wing length of male juvenile birds is 63.84 and female wings are 3.35 shorter given that they belong to the same agegroup. Adult wing length is 0.88 longer than juvenile length, given that we compare birds from the same sex. However, as we already could see in the plot, the difference between juvenile and adult is different depending on sex. Let us see what happens if we include the interaction $sex \times age$:

```{r}
pmod2 <- lm('wing ~ sex * age', df)
summary(pmod2)
amod <- aov(wing ~ sex * age, df)
summary(amod)
```

```{r echo=FALSE}
aovprint <- apa_print.aov(amod)
```



$$wing=63.5 -2.67female + 1.33adu -1.04fem \times adu$$

This means: In our data, the mean wing length of male, juvenile birds is 63.5. Male adults have 1.33 longer wings than that. Female, juvenile birds have 2.67 shorter wing length compared to juvenile males and female adult birds have 0.29 shorter wing length than juvenile male birds. As you can see, for the two-way Anova, I used the function _aov_, which conducts sequential F-test. This type of testing delivers the following output:

`r print(aovprint$table)`

 First the function _aov_ tests whether _sex_ explains a significant amount of variance over the baseline model. Then it checks whether _age_ explains significant amount of variance when added to the model where _sex_ is already implemented and so forth... Another method would be marginal F-tests, which tests whether any of the variables explains a significant amount of variance when all other variables are already in the model. It is important to know that the order in which we enter the variables in the function determines in which order the sequential analysis works. In case, the F-test yields a significant result, post hoc test could be conducted to further analyze which group means differ. 
 In the Bayesian framework we have a 5% chance that the true value lies outside the credible interval (CrI) every time we calculate it, so there is also a similar problem. Personally, I will stick with the following advice given by @Gelman2007: 

 > - Formulate scientific hypotheses prior to data collection and stick to these during data analyses
 - Use, whenever possible hierarchical models (mixed models)
 - Accept that the true parameter value is outside the 95% CrI in 5% of the cases
 - Be honest - that is, describe the search effort that has been spent to find significant results, and also publish the nonsignificant results


## ANCOVA 
ANCOVA tries to answer questions like: 'Are there differences between means of the groups after adjusting for a covariate?'. @Field2012 explains that the covariate is a continuous variable that is thought to reduce the RSS of the model, thereby increasing the power of the F-test. However, we also have one df less for each covariate that we include in the model. Therefore, including covariates in the model always comes at a trade-off. @Tabachnick2007 notes that the interpretation of _adjusted mean_ is misleading because it may not correspond to any real life situation: Adjusted means would mean that all subjects had the same score on the covariate. That is not realistic, especially in non-experimental settings. So they conclude:  
> At best, the nonexperimental use of ANCOVA allows you to look at IV-DV relationships adjusted for the effects of CVs.

@Korner-Nievergelt2015 give an example where the (log) biomass of two grass species is compared, while taking into consideration the average distance to ground water. However, they do not invest a lot into this model within the context of ANCOVA since they will analyze the same data in a mixed model design later. 

```{r}
data(ellenberg)
df <- ellenberg[is.element(ellenberg$Species, c('Ap', 'Dg')),]
df <- droplevels(df)
str(df)
#center cov 
df$c_water <- scale(df$Water, center=T, scale=F)
df$l_biomass <- log(df$Yi.g)
ggplot(df, aes(x=c_water, y=l_biomass, color=Species)) +
geom_point() +
geom_smooth(method='lm')

mod <- lm(l_biomass ~Species * c_water, data=df)
summary(mod)
head(model.matrix(mod))
```

We get:

$$log(biomass) = 3.16 - 0.24SpeciesDg - 0.02water + 0.02SpeciesDf \times water$$

A useful question in ecology that we could try to answer with this model is at which water level species Dg will outperform Ap. This is at the crosspoint in the above graph. 







## Lecture

#### Planned contrast
We can specify beforehand that we are only interested in testing the group differences between certain groups and leave out others. 

```{r}
#1. Test assumptions
#Independence of residuals
#homogeneity of Variance between groups 
#normal distributions within groups

#2. specify contrasts
#3. perform analysis 
#4. check models diagnostics 
#5. Examine results


```




## Notes for contrasts in R:
* there are only k-1 orthogonal comparisons (where k is the number of factor levels)
* that means these are statistical independent 
* choose contrasts based on your hypothesis before data collection
* 




# Field and Bill for exam 


Play around with contrasts and check whether the order of input matters when using type I ss as Bill said..

## One way 

```{r}
library(multcomp)
library(compute.es)
library(pastecs)
```

### Assumptions

#### Homogeneity of variance 

```{r}
leveneTest()
```

#### Independence of observations

```{r}
dwt()
```

#### Normal distribution within groups

```{r}
by()
```

### Post Hoc Tests 
- with no specific hypothesis before the experiment, use post hoc tests (otherwise use planned contrasts)
- with equal sample sizes use Tukey
- if you want guarenteed control over type I error rate, use Bonferroni

```{r}
#Tukey --> Multcomp
object <- glht(model, linfct=mcp(dose='Tukey'))
summary(object)

#Bonferroni
pairwise.t.test(datavector, category, p.adjust.method='bonderroni')
pairwise.t.test(datavector, category, p.adjust.method='BH')
```

### Effect sizes

Report effect sizes instead of p-values because of the known advantages. The function below can report multiple effect sizes if you put in arguments as shown:

```{r}
mes()
```

## ANCOVA 

### Assumptions 
- Independence of covariate and IV
  - We do not want the IV and the covariate to be correlated because it would otherwise explain variance that the IV might explain. We want to remove error variance but not take away variance the IV actually can explain.
- Linearity between covariate and DV
- Homogeneity of regression slopes
  - The relationship between the covariate and DV should be similar for all groups

## Factorial ANOVA 

### Assumptions 

**Homogeneity of variance:**  
Same as before but also check for the interaction! 

```{r}
##you can do it this way 
#leveneTest(gogglesData$attractiveness, interaction(gogglesData$alcohol, gogglesData$gender))
##but then you have to run two other lines as well for the main effects. So you could also do:
#leveneTest(model)
```

**Independence of observations:**  
Should be fulffilled by definition in experimental design

**Normality of residuals WITHIN groups:**  


## Repeated measures 

### Assumptions 

**Sphericity:**  
Check out if the the variance of the difference between pre/post, pre/follow up and post/follow up are homogeneous. If not, then the assumption of sphericity is violated (which is generally the case). This makes of course only sense if you have more than 2 groups. To test this assumption use Mauchly test. If significant report adjusted F-ratios (e.g. Greenhouse G. adjusted).

**Independence of Errors:**  
Violated by definition in repeated measures. 

### Lord' Paradox
- Depending on whether you look for relative or absolute change you may find a difference or not.
- Randomization check: If you find significant differences in the outcome in a pre condition between groups of subjects (example do males gain more weight than females on a specific diet over time), you might better choose the covariate approach to account for the difference whereas you better choose the difference score approach (repreated measures) when there are no differences in the start.


## MANOVA 

use `cbind` to to create a object that you use in the function `manova`. 

### Assumptions

**Homogeneity of variance-covariance matrices:**  
The Box-M test tests all the assumptions (homogeneity of varicance and homogeneity of covariance) at once. But it is very senseitive so you set the p-value at <.001.

**Independence of observations:**  
Of course not for the within subjects effects but since you can have both within and between in MANOVA, this assumptions needs to be met for between subjects effects.

**Multivariate normality:**  
Since we have multiple DV, we have to look if all are normally distributed:  
- Shapiro-Wilks multivariate test in mvnormtest package 
- mardia's test in package MVN 
  - skewness/kurtosis
  - output in plain english what the outcome was 


**Multivariate outliers:**  
- ac.plot from package mvoutlier provides colorcoding of potential outliers 
- 





The 5 steps in Anova
1. Test Assumptions
2. specify contrasts
3. Perform analysis
4. check model diagnostics
5. examine results



## Type sum of squares

type 1 (sequential):
- Order matters
- step by step and the next can only explain variance the first one did not explain

type 2:
- the main effects are prioritized over interactions. First variance explained by main effects, then interaction. So, if you are interested in interactions, don't use this but this is most important for main effects

type 3 (marginal or weighted):
- All effects have equal weight. When interactions are of interest this is the way to go (order does not matter).



Options to model the complexer designs



## First option: Ez Anova

between and/or within participant effects

```{r}
library(ez)
#ezAnova(data = df, dv= .(DV), wid = .(id), within = (IV1, IV2), between = .(IV3, IV4), type = 3, detailed= FALSE)
```
## Second option: aov_ez, aov_4, aov_car (afex package)

This package has type III ss as default and for between subjects effects it uses zero sum contrasts.
```{r}
library(afex)
aov_ez('id',)
```

Alternative to pairwise t-test for obtaining pairwise comparisons is the least squares means method (lsmeans). Tests pairwise using Tukey correction.


The general linear model can be applies to all ANOVA techniques but I should know format requirements for the single techniques, e.g.:
- a within design needs long format 
- afex() and ezAnova are most flexible functions
- lm() can only accommodate one DV (or one aggregate)

Zero-sum codes are preferable
- specify and interpret accordingly

ANOVAs can be performed using various R packages
- aov(type I ss), Anova(type III ss out ob aov object), anova(compare nested models), ezAnova & manova



What are the different functions we have used to calculate correlations?

```{r echo=FALSE}
#simulate dataset
b0 <- 12 #true mean group 1
b1 <- 1.5 #difference in true means between group 2 to group 1
b2 <- -2 #difference in true means between group 1 and group 3
sigma <- 2 #residual SD (=within group SD)
n <- 90
group <- factor(c(rep('a', 20), rep('b', 40), rep('c', 30)))
#simulate the y variable
simresid <- rnorm(n, mean=0, sd=sigma)
y <- b0 + as.numeric(group=='2') *b1 + as.numeric(group=='3')*b2 + simresid
df <- data.frame(group, y)
```

```{r}
x <- rnorm(30, mean=10, sd= 2)
y <- rnorm(30, mean=13, sd= 2)
cor(x,y)
cor.test(x,y)
#psych
corr.test(data.frame(x,y))
library(ltm)
rcor.test(data.frame(x,y))

library(Hmisc)
rcorr(x,y)
```


### Play with effect codes

```{r}
#for dummy codes it is clear: Intercept=mean, and coefs = diffs to mean 
contrasts(df$group) <- contr.treatment
contrasts(df$group)
m1 <- lm(y ~ group, df)
summary(m1)
```

```{r}
contrasts(df$group) <- contr.sum
contrasts(df$group)
#I expect the intercept to be the unweighted grand mean, the first coef reflects the diff of gm to mean group a, the second coef to mean group b. Group c is not reflected by the coefs 
m2 <- lm(y~group, df)
summary(m2)
by(df$y, df$group, mean)


library(car)
Anova(m1, type=3)
Anova(m2, type=3)
```


## References